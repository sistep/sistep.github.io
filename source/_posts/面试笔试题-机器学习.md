---
title: 面试笔试题-机器学习
date: 2017-09-24 21:28:27
tags:
- 机器学习
categories:
- 面试笔试
description: SVM、决策树、AdaBoost
toc: true
---

# 过拟合与欠拟合

[差与方差，欠拟合与过拟合](http://blog.csdn.net/hurry0808/article/details/78148756)

# SVM

[支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/v_july_v/article/details/7624837)

# 决策树、SVM、AdaBoost方法的比较

参考内容见[CSDN博客](http://blog.csdn.net/sinat_32547403/article/details/72911193)

# 交叉熵与相对熵（KL距离）

作者：知乎用户
链接：https://www.zhihu.com/question/41252833/answer/108777563
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

熵的本质是香农信息量()的期望。现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布。按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：H(p)=。如果使用错误分布q来表示来自真实分布p的平均编码长度，则应该是：H(p,q)=。因为用q来编码的样本来自分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(当然还有C和D，尽管C和D并不会出现，因为真实分布p中C和D出现的概率为0，这里就钦定概率为0的事件不会发生啦)。可以看到上例中根据非真实分布q得到的平均编码长度H(p,q)大于根据真实分布p得到的平均编码长度H(p)。事实上，根据Gibbs' inequality可知，H(p,q)>=H(p)恒成立，当q为真实分布p时取等号。我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“相对熵”：D(p||q)=H(p,q)-H(p)=，其又被称为KL散度(Kullback–Leibler divergence，KLD) Kullback–Leibler divergence。它表示2个函数或概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小，特别地，若2者相同则熵为0。注意，KL散度的非对称性。比如TD-IDF算法就可以理解为相对熵的应用：词频在整个语料库的分布与词频在具体文档中分布之间的差异性。交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。PS：通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。
